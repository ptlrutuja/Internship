{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8e6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e7654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d89ce35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Rank                        NAme  Artist  Upload_date\n",
      "0           0   NaN                music videos   False        False\n",
      "1           1   NaN            Baby Shark Dance   False        False\n",
      "2           2   NaN        Johny Johny Yes Papa   False        False\n",
      "3           3   NaN  Cocomelon – Nursery Rhymes   False        False\n",
      "4           4   NaN  Cocomelon – Nursery Rhymes   False        False\n",
      "5           5   NaN  Cocomelon – Nursery Rhymes   False        False\n",
      "6           6   NaN            Baby Shark Dance   False        False\n",
      "7           7   NaN            Baby Shark Dance   False        False\n",
      "8           8   NaN        Johny Johny Yes Papa   False        False\n",
      "9           9   NaN            Baby Shark Dance   False        False\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "#element = driver.find_element_by_xpath(\"//a[@class='mw-redirect']\")\n",
    "#element.text\n",
    "Ranks = []\n",
    "Names = []\n",
    "Artists = []\n",
    "Upload_dates = []\n",
    "View = []\n",
    "\n",
    "rank_tag=driver.find_elements_by_xpath('td')\n",
    "for i in rank_tag[0:10]:\n",
    "    Ranks.append(i.text)\n",
    "\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='mw-redirect']\")\n",
    "for i in titles[0:10]:\n",
    "    Names.append(i.text)\n",
    "    \n",
    "Artists_tags=driver.find_elements_by_xpath(\"//th[@class='headerSort headerSortUp']\")\n",
    "for i in Artists_tags[0:10]:\n",
    "    Artists.append(i.text)\n",
    "    \n",
    "Upload_dates_tag=driver.find_elements_by_xpath('td')\n",
    "for i in Upload_dates_tag[0:10]:\n",
    "    Upload_dates.append(i.text)\n",
    "\n",
    "youtube_data=pd.DataFrame({})\n",
    "youtube_data['Rank']=Ranks\n",
    "youtube_data['NAme']= Names\n",
    "youtube_data['Artist']= Artists == 10\n",
    "youtube_data['Upload_date']=Upload_dates == 10\n",
    "#youtube_data['Location']=Locations\n",
    "\n",
    "\n",
    "convert = pd.DataFrame(youtube_data)\n",
    "convert.to_csv(\"youtube_data.csv\")\n",
    "\n",
    "a = pd.read_csv(\"youtube_data.csv\")\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67f1047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "try:\n",
    "    search_recruiters = driver.find_element_by_xpath(\"//div[@class='mTxt']\")\n",
    "    search_recruiters.click()\n",
    "except ElementNotInteractableException as e:\n",
    "    print(\"exception raised\" , e)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d546a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0              Name                        Designation  Company  \\\n",
      "0           0         CodeFlies                    Human Resources    False   \n",
      "1           1    Santhosh Kumar  Associate Manager Human Resources    False   \n",
      "2           2        Surendra R                  Company Recruiter    False   \n",
      "3           3      Gauri Bhosle                    Human Resources    False   \n",
      "4           4           Sanghvi                  Company Recruiter    False   \n",
      "5           5  Rohit Khandelwal                  Company Recruiter    False   \n",
      "6           6       Neha Pandey                  Company Recruiter    False   \n",
      "7           7       Vinita Raut                  Company Recruiter    False   \n",
      "8           8        sunny rana                          Recruiter    False   \n",
      "9           9            Snehal                       HR cum Admin    False   \n",
      "\n",
      "              Skill          Location  \n",
      "0         CodeFlies         CodeFlies  \n",
      "1    Santhosh Kumar    Santhosh Kumar  \n",
      "2        Surendra R        Surendra R  \n",
      "3      Gauri Bhosle      Gauri Bhosle  \n",
      "4           Sanghvi           Sanghvi  \n",
      "5  Rohit Khandelwal  Rohit Khandelwal  \n",
      "6       Neha Pandey       Neha Pandey  \n",
      "7       Vinita Raut       Vinita Raut  \n",
      "8        sunny rana        sunny rana  \n",
      "9            Snehal            Snehal  \n"
     ]
    }
   ],
   "source": [
    "url= \"https://www.naukri.com/hr-recruiters-consultants\"\n",
    "driver.get(url)\n",
    "\n",
    "Names=[]\n",
    "Designations=[]\n",
    "Companys=[]\n",
    "Skills=[]\n",
    "Locations=[]\n",
    "\n",
    "\n",
    "titles=driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\")\n",
    "for i in titles[0:10]:\n",
    "    Names.append(i.text)\n",
    "\n",
    "Designations_tag=driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\")\n",
    "for i in Designations_tag[0:10]:\n",
    "    Designations.append(i.text)\n",
    "    \n",
    "Companys_tag=driver.find_elements_by_xpath(\"//span[@class='ellipsis']\")\n",
    "for i in Companys_tag[0:10]:\n",
    "    Companys.append(i.text)\n",
    "\n",
    "Skills_tag=driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\")\n",
    "for i in Skills_tag[0:10]:\n",
    "    Skills.append(i.text)\n",
    "\n",
    "Locations_tags=driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\")\n",
    "for i in Locations_tags[0:10]:\n",
    "    Locations.append(i.text)   \n",
    "    \n",
    "data=pd.DataFrame({})\n",
    "data['Name']=Names\n",
    "data['Designation']= Designations\n",
    "data['Company']= Companys == 10\n",
    "data['Skill']=Skills\n",
    "data['Location']=Locations\n",
    "\n",
    "\n",
    "convert = pd.DataFrame(data)\n",
    "convert.to_csv(\"output1.csv\")\n",
    "\n",
    "a = pd.read_csv(\"output1.csv\")\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48b4a6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                    Name    Year_span  \\\n",
      "0           0          1. Game of Thrones (2011–2019)  (2011–2019)   \n",
      "1           1             2. Stranger Things (2016– )     (2016– )   \n",
      "2           2         3. The Walking Dead (2010–2022)  (2010–2022)   \n",
      "3           3           4. 13 Reasons Why (2017–2020)  (2017–2020)   \n",
      "4           4                  5. The 100 (2014–2020)  (2014–2020)   \n",
      "5           5  6. Orange Is the New Black (2013–2019)  (2013–2019)   \n",
      "6           6                   7. Riverdale (2017– )     (2017– )   \n",
      "7           7              8. Grey's Anatomy (2005– )     (2005– )   \n",
      "8           8                   9. The Flash (2014– )     (2014– )   \n",
      "9           9                   10. Arrow (2012–2020)  (2012–2020)   \n",
      "\n",
      "                      Genre Run_times Ratings                  Votes  \n",
      "0  Action, Adventure, Drama    57 min     9.2  created - 10 Oct 2019  \n",
      "1    Drama, Fantasy, Horror    51 min     NaN  updated - 10 Oct 2019  \n",
      "2   Drama, Horror, Thriller    44 min    Rate                 Votes:  \n",
      "3  Drama, Mystery, Thriller    60 min     NaN                 Votes:  \n",
      "4    Drama, Mystery, Sci-Fi    43 min     NaN                 Votes:  \n",
      "5      Comedy, Crime, Drama    59 min     NaN                 Votes:  \n",
      "6     Crime, Drama, Mystery    45 min     NaN                 Votes:  \n",
      "7            Drama, Romance    41 min     NaN                 Votes:  \n",
      "8  Action, Adventure, Drama    43 min     NaN                 Votes:  \n",
      "9  Action, Adventure, Crime    42 min     NaN                 Votes:  \n"
     ]
    }
   ],
   "source": [
    "#Q9\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://www.imdb.com/list/ls095964455/')\n",
    "\n",
    "\n",
    "\n",
    "Names = []\n",
    "Year_spans = []\n",
    "Genres = []\n",
    "Run_time = []\n",
    "Rating = []\n",
    "Vote =[]\n",
    "\n",
    "\n",
    "titles=driver.find_elements_by_xpath(\"//h3[@class ='lister-item-header']\")\n",
    "for i in titles[0:10]:\n",
    "    Names.append(i.text)\n",
    "    \n",
    "Year_spans_tag =driver.find_elements_by_xpath(\"//span[@class ='lister-item-year text-muted unbold']\")\n",
    "for i in Year_spans_tag[0:10]:\n",
    "    Year_spans.append(i.text)\n",
    "    \n",
    "    \n",
    "Genres_tag =driver.find_elements_by_xpath(\"//span[@class ='genre']\")\n",
    "for i in Genres_tag[0:10]:\n",
    "    Genres.append(i.text)\n",
    "    \n",
    "Run_time_tag =driver.find_elements_by_xpath(\"//span[@class ='runtime']\")\n",
    "for i in Run_time_tag[0:10]:\n",
    "    Run_time.append(i.text)\n",
    "    \n",
    "Rating_tag =driver.find_elements_by_xpath(\"//span[@class ='ipl-rating-star__rating']\")\n",
    "for i in Rating_tag[0:10]:\n",
    "    Rating.append(i.text)\n",
    "    \n",
    "Vote_tag =driver.find_elements_by_xpath(\"//span[@class ='text-muted']\")\n",
    "for i in Vote_tag[0:10]:\n",
    "    Vote.append(i.text)\n",
    "    \n",
    "imdb_data=pd.DataFrame({})\n",
    "imdb_data['Name']=Names\n",
    "imdb_data['Year_span']= Year_spans\n",
    "imdb_data['Genre']= Genres\n",
    "imdb_data['Run_times']=Run_time\n",
    "imdb_data['Ratings']=Rating\n",
    "imdb_data['Votes']=Vote\n",
    "\n",
    "\n",
    "convert = pd.DataFrame(imdb_data)\n",
    "convert.to_csv(\"imdb_data.csv\")\n",
    "\n",
    "a = pd.read_csv(\"imdb_data.csv\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd127b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception raised Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//a[@href='datasets.php']\"}\n",
      "  (Session info: chrome=97.0.4692.71)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00306903+2517251]\n",
      "\tOrdinal0 [0x0029F8E1+2095329]\n",
      "\tOrdinal0 [0x001A2848+1058888]\n",
      "\tOrdinal0 [0x001CD448+1233992]\n",
      "\tOrdinal0 [0x001CD63B+1234491]\n",
      "\tOrdinal0 [0x001F7812+1406994]\n",
      "\tOrdinal0 [0x001E650A+1336586]\n",
      "\tOrdinal0 [0x001F5BBF+1399743]\n",
      "\tOrdinal0 [0x001E639B+1336219]\n",
      "\tOrdinal0 [0x001C27A7+1189799]\n",
      "\tOrdinal0 [0x001C3609+1193481]\n",
      "\tGetHandleVerifier [0x00495904+1577972]\n",
      "\tGetHandleVerifier [0x00540B97+2279047]\n",
      "\tGetHandleVerifier [0x00396D09+534521]\n",
      "\tGetHandleVerifier [0x00395DB9+530601]\n",
      "\tOrdinal0 [0x002A4FF9+2117625]\n",
      "\tOrdinal0 [0x002A98A8+2136232]\n",
      "\tOrdinal0 [0x002A99E2+2136546]\n",
      "\tOrdinal0 [0x002B3541+2176321]\n",
      "\tBaseThreadInitThunk [0x75CCFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77A87A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77A87A6E+238]\n",
      "\t(No symbol) [0x00000000]\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Dataset_name, Data_type, Task, Attribute_type, No_of_instance, No_of_attribute, Year]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q10\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://archive.ics.uci.edu/')\n",
    "\n",
    "try:\n",
    "    search_btn = driver.find_element_by_xpath(\"//a[@href='datasets.php']\")\n",
    "    search_btn.click()\n",
    "except NoSuchElementException as e:\n",
    "    print(\"exception raised\" , e)\n",
    "\n",
    "    \n",
    "Dataset_names = []\n",
    "Data_types = []\n",
    "Tasks= []\n",
    "Attribute_types= []\n",
    "No_of_instances= []\n",
    "No_of_attributes= []\n",
    "Years= []\n",
    "\n",
    "\n",
    "titles=driver.find_elements_by_xpath(\"//p[@class ='normal']\")\n",
    "for i in titles[0:10]:\n",
    "    Dataset_names.append(i.text)\n",
    "    \n",
    "    \n",
    "Data_types_tag = driver.find_elements_by_xpath(\"//p[@class ='normal']\")\n",
    "for i in titles[0:10]:\n",
    "    Data_types.append(i.text)\n",
    "    \n",
    "Tasks_tag=driver.find_elements_by_xpath(\"//p[@class ='normal']\")\n",
    "for i in Tasks_tag[0:10]:\n",
    "    Tasks.append(i.text)\n",
    "    \n",
    "Attribute_types_tag=driver.find_elements_by_xpath(\"//p[@class ='normal']\")\n",
    "for i in Attribute_types_tag[0:10]:\n",
    "    Attribute_types.append(i.text)\n",
    "    \n",
    "No_of_instances_tag=driver.find_elements_by_xpath(\"//p[@class ='normal']\")\n",
    "for i in No_of_instances_tag[0:10]:\n",
    "    No_of_instances.append(i.text)\n",
    "\n",
    "    \n",
    "No_of_attributes_tag=driver.find_elements_by_xpath(\"//p[@class ='normal']\")\n",
    "for i in No_of_attributes_tag[0:10]:\n",
    "    No_of_attributes.append(i.text)\n",
    "    \n",
    "\n",
    "Years_tag=driver.find_elements_by_xpath(\"//p[@class ='normal']\")\n",
    "for i in Years_tag[0:10]:\n",
    "    Years.append(i.text)\n",
    "\n",
    "ics_data=pd.DataFrame({})\n",
    "ics_data['Dataset_name']= Dataset_names\n",
    "ics_data['Data_type']= Data_types\n",
    "ics_data['Task']= Tasks\n",
    "ics_data['Attribute_type']= Attribute_types\n",
    "ics_data['No_of_instance']= No_of_instances\n",
    "ics_data['No_of_attribute']= No_of_attributes\n",
    "ics_data['Year']= Years\n",
    "\n",
    "\n",
    "convert = pd.DataFrame(ics_data)\n",
    "convert.to_csv(\"ics_data.csv\")\n",
    "\n",
    "a = pd.read_csv(\"ics_data.csv\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fafe3752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Brown, Dan   5,094,805   Transworld   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "2  Rowling, J.K.   4,475,152   Bloomsbury   Children's Fiction   Children's Fiction \n",
      "3  Rowling, J.K.   4,200,654   Bloomsbury   Children's Fiction   Children's Fiction \n",
      "4  Rowling, J.K.   4,179,479   Bloomsbury   Children's Fiction   Children's Fiction \n",
      "5  James, E. L.   3,758,936   Random House   Romance & Sagas   Romance & Sagas \n",
      "6  Rowling, J.K.   3,583,215   Bloomsbury   Children's Fiction   Children's Fiction \n",
      "7  Rowling, J.K.   3,484,047   Bloomsbury   Children's Fiction   Children's Fiction \n",
      "8  Rowling, J.K.   3,377,906   Bloomsbury   Children's Fiction   Children's Fiction \n",
      "9  Brown, Dan   3,193,946   Transworld   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "10  Rowling, J.K.   2,950,264   Bloomsbury   Children's Fiction   Children's Fiction \n",
      "11  James, E. L.   2,479,784   Random House   Romance & Sagas   Romance & Sagas \n",
      "12  Meyer, Stephenie   2,315,405   Little, Brown Book   Young Adult Fiction   Young Adult Fiction \n",
      "13  Larsson, Stieg   2,233,570   Quercus   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "14  James, E. L.   2,193,928   Random House   Romance & Sagas   Romance & Sagas \n",
      "15  Brown, Dan   2,183,031   Transworld   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "16  Meyer, Stephenie   2,152,737   Little, Brown Book   Young Adult Fiction   Young Adult Fiction \n",
      "17  Brown, Dan   2,062,145   Transworld   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "18  Meyer, Stephenie   2,052,876   Little, Brown Book   Young Adult Fiction   Young Adult Fiction \n",
      "19  Sebold, Alice   2,005,598   Pan Macmillan   General & Literary Fiction   General & Literary Fiction \n",
      "20  Haddon, Mark   1,979,552   Random House   General & Literary Fiction   General & Literary Fiction \n",
      "21  Brown, Dan   1,928,900   Transworld   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "22  Bryson, Bill   1,852,919   Transworld   Popular Science   Popular Science \n",
      "23  Larsson, Stieg   1,814,784   Quercus   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "24  Meyer, Stephenie   1,787,118   Little, Brown Book   Young Adult Fiction   Young Adult Fiction \n",
      "25  Carle, Eric   1,783,535   Penguin   Picture Books   Picture Books \n",
      "26  Donaldson, Julia   1,781,269   Pan Macmillan   Picture Books   Picture Books \n",
      "27  Oliver, Jamie   1,743,266   Penguin   Food & Drink: General   Food & Drink: General \n",
      "28  Hosseini, Khaled   1,629,119   Bloomsbury   General & Literary Fiction   General & Literary Fiction \n",
      "29  Nicholls, David   1,616,068   Hodder & Stoughton   General & Literary Fiction   General & Literary Fiction \n",
      "30  Hosseini, Khaled   1,583,992   Bloomsbury   General & Literary Fiction   General & Literary Fiction \n",
      "31  Larsson, Stieg   1,555,135   Quercus   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "32  Niffenegger, Audrey   1,546,886   Random House   General & Literary Fiction   General & Literary Fiction \n",
      "33  McEwan, Ian   1,539,428   Random House   General & Literary Fiction   General & Literary Fiction \n",
      "34  Fielding, Helen   1,508,205   Pan Macmillan   General & Literary Fiction   General & Literary Fiction \n",
      "35  Clarkson, Jeremy   1,489,403   Penguin   Humour: Collections & General   Humour: Collections & General \n",
      "36  Bernieres, Louis de   1,352,318   Random House   General & Literary Fiction   General & Literary Fiction \n",
      "37  Kay, Peter   1,310,207   Random House   Autobiography: General   Autobiography: General \n",
      "38  Martel, Yann   1,310,176   Canongate   General & Literary Fiction   General & Literary Fiction \n",
      "39  Stephenson, Pamela   1,231,957   HarperCollins   Biography: The Arts   Biography: The Arts \n",
      "40  Pelzer, Dave   1,217,712   Orion   Autobiography: General   Autobiography: General \n",
      "41  Donaldson, Julia   1,208,711   Pan Macmillan   Picture Books   Picture Books \n",
      "42  McCourt, Frank   1,204,058   HarperCollins   Autobiography: General   Autobiography: General \n",
      "43  Faulks, Sebastian   1,184,967   Random House   General & Literary Fiction   General & Literary Fiction \n",
      "44  Pullman, Philip   1,181,503   Scholastic Ltd.   Young Adult Fiction   Young Adult Fiction \n",
      "45  Mosse, Kate   1,181,093   Orion   General & Literary Fiction   General & Literary Fiction \n",
      "46  Rowling, J.K.   1,153,181   Bloomsbury   Science Fiction & Fantasy   Science Fiction & Fantasy \n",
      "47  Stockett, Kathryn   1,132,336   Penguin   General & Literary Fiction   General & Literary Fiction \n",
      "48  Parsons, Tony   1,130,802   HarperCollins   General & Literary Fiction   General & Literary Fiction \n",
      "49  Golden, Arthur   1,126,337   Random House   General & Literary Fiction   General & Literary Fiction \n",
      "50  McCall Smith, Alexander   1,115,549   Little, Brown Book   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "51  Hislop, Victoria   1,108,328   Headline   General & Literary Fiction   General & Literary Fiction \n",
      "52  Ahern, Cecelia   1,107,379   HarperCollins   General & Literary Fiction   General & Literary Fiction \n",
      "53  McKeith, Gillian   1,104,403   Penguin   Fitness & Diet   Fitness & Diet \n",
      "54  Zafon, Carlos Ruiz   1,092,349   Orion   General & Literary Fiction   General & Literary Fiction \n",
      "55  Rowling, J.K.   1,090,847   Bloomsbury   Children's Fiction   Children's Fiction \n",
      "56  Grisham, John   1,087,262   Random House   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "57  Atkins, Robert C.   1,054,196   Random House   Fitness & Diet   Fitness & Diet \n",
      "58  Pullman, Philip   1,037,160   Scholastic Ltd.   Young Adult Fiction   Young Adult Fiction \n",
      "59  Truss, Lynne   1,023,688   Profile Books Group   Usage & Writing Guides   Usage & Writing Guides \n",
      "60  Smith, Delia   1,015,956   Random House   Food & Drink: General   Food & Drink: General \n",
      "61  Harris, Joanne   1,009,873   Transworld   General & Literary Fiction   General & Literary Fiction \n",
      "62  Boyne, John   1,004,414   Random House Childrens Books G   Young Adult Fiction   Young Adult Fiction \n",
      "63  Picoult, Jodi   1,003,780   Hodder & Stoughton   General & Literary Fiction   General & Literary Fiction \n",
      "64  Pullman, Philip   1,002,314   Scholastic Ltd.   Young Adult Fiction   Young Adult Fiction \n",
      "65  Lee, Harper   998,213   Random House   General & Literary Fiction   General & Literary Fiction \n",
      "66  Gray, John   992,846   HarperCollins   Popular Culture & Media: General Interest   Popular Culture & Media: General Interest \n",
      "67  French, Dawn   986,753   Random House   Autobiography: The Arts   Autobiography: The Arts \n",
      "68  Lewycka, Marina   986,115   Penguin   General & Literary Fiction   General & Literary Fiction \n",
      "69  Harris, Thomas   970,509   Random House   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "70  Tolkien, J. R. R.   967,466   HarperCollins   Science Fiction & Fantasy   Science Fiction & Fantasy \n",
      "71  Moore, Michael   963,353   Penguin   Current Affairs & Issues   Current Affairs & Issues \n",
      "72  Rubenfeld, Jed   962,515   Headline   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "73  Osbourne, Sharon   959,496   Little, Brown Book   Autobiography: The Arts   Autobiography: The Arts \n",
      "74  Coelho, Paulo   956,114   HarperCollins   General & Literary Fiction   General & Literary Fiction \n",
      "75  O'Grady, Paul   945,640   Transworld   Autobiography: The Arts   Autobiography: The Arts \n",
      "76  Bryson, Bill   931,312   Transworld   Travel Writing   Travel Writing \n",
      "77  Oliver, Jamie   925,425   Penguin   Food & Drink: General   Food & Drink: General \n",
      "78  Fielding, Helen   924,695   Pan Macmillan   General & Literary Fiction   General & Literary Fiction \n",
      "79  Oliver, Jamie   906,968   Penguin   National & Regional Cuisine   National & Regional Cuisine \n",
      "80  McKenna, Paul   905,086   Transworld   Fitness & Diet   Fitness & Diet \n",
      "81  Bryson, Bill   890,847   Transworld   Travel Writing   Travel Writing \n",
      "82  Grisham, John   869,671   Random House   Crime, Thriller & Adventure   Crime, Thriller & Adventure \n",
      "83  Levy, Andrea   869,659   Headline   General & Literary Fiction   General & Literary Fiction \n",
      "84  Lawson, Nigella   862,602   Random House   Food & Drink: General   Food & Drink: General \n",
      "85  Ali, Monica   856,540   Transworld   General & Literary Fiction   General & Literary Fiction \n",
      "86  Edwards, Kim   845,858   Penguin   General & Literary Fiction   General & Literary Fiction \n",
      "87  Donaldson, Julia   842,535   Pan Macmillan   Picture Books   Picture Books \n",
      "88  Hornby, Nick   828,215   Penguin   General & Literary Fiction   General & Literary Fiction \n",
      "89  Brand, Russell   820,563   Hodder & Stoughton   Autobiography: The Arts   Autobiography: The Arts \n",
      "90  Dawkins, Richard   816,907   Transworld   Popular Science   Popular Science \n",
      "91  0   816,585   D.C. Thomson   Children's Annuals   Children's Annuals \n",
      "92  Smith, Zadie   815,586   Penguin   General & Literary Fiction   General & Literary Fiction \n",
      "93  Morton, Kate   814,370   Pan Macmillan   General & Literary Fiction   General & Literary Fiction \n",
      "94  Zusak, Markus   809,641   Transworld   General & Literary Fiction   General & Literary Fiction \n",
      "95  Binchy, Maeve   808,900   Orion   General & Literary Fiction   General & Literary Fiction \n",
      "96  Harris, Robert   807,311   Random House   General & Literary Fiction   General & Literary Fiction \n",
      "97  Oliver, Jamie   794,201   Penguin   Food & Drink: General   Food & Drink: General \n",
      "98  Collins, Suzanne   792,187   Scholastic Ltd.   Young Adult Fiction   Young Adult Fiction \n",
      "99  Pelzer, Dave   791,507   Orion   Biography: General   Biography: General \n",
      "100  Oliver, Jamie   791,095   Penguin   Food & Drink: General   Food & Drink: General \n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Rank, Name, Author, Volumes, Publisher_name, Genres]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q8\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get('https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/')\n",
    "\n",
    "url = 'https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "book_table= soup.find('table',class_ = 'in-article sortable')\n",
    "\n",
    "for team in book_table.find_all('tbody'):\n",
    "    rows = team.find_all('tr')\n",
    "    for row in rows:\n",
    "        book_rank = row.find('td', class_ = 'left').text.strip()\n",
    "        book_name=row.find_all('td',class_ = 'left')[2].text\n",
    "        Author_name=row.find_all('td',class_ = 'left')[3].text\n",
    "        Volumes_sold=row.find_all('td',class_ = 'left')[4].text\n",
    "        Publisher=row.find_all('td',class_ = 'left')[5].text\n",
    "        Genre=row.find('td',class_ = 'last left').text\n",
    "        print(book_rank,book_name,Author_name,Volumes_sold,Publisher,Genre)\n",
    "\n",
    "        \n",
    "book_data=pd.DataFrame({})\n",
    "book_data['Rank']= book_rank\n",
    "book_data['Name']= book_name\n",
    "book_data['Author']= Author_name\n",
    "book_data['Volumes']= Volumes_sold\n",
    "book_data['Publisher_name']= Publisher\n",
    "book_data['Genres']= Genre\n",
    "\n",
    "convert = pd.DataFrame(book_data)\n",
    "convert.to_csv(\"book_data.csv\")\n",
    "\n",
    "a = pd.read_csv(\"book_data.csv\")\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2545e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Match_title, Series, Place, Date, Time]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://www.bcci.tv/')\n",
    "\n",
    "\n",
    "Match_titles=[]\n",
    "Seriess=[]\n",
    "Places=[]\n",
    "Dates=[]\n",
    "Times=[]\n",
    "\n",
    "\n",
    "titles=driver.find_elements_by_xpath(\"//div[@class ='match-bottom-info ']\")\n",
    "for i in titles[0:10]:\n",
    "    Match_titles.append(i.text)\n",
    "    \n",
    "    \n",
    "Seriess_tag=driver.find_elements_by_xpath(\"//span[@class ='ng-binding']\")\n",
    "for i in Seriess_tag[0:10]:\n",
    "    Seriess.append(i.text)\n",
    "    \n",
    "Places_tag=driver.find_elements_by_xpath(\"//span[@class ='ng-binding ng-scope']\")\n",
    "for i in Places_tag[0:10]:\n",
    "    Places.append(i.text)\n",
    "\n",
    "Dates_tags=driver.find_elements_by_xpath(\"//div[@class ='match-card-left match-schedule']\")\n",
    "for i in Dates_tags[0:10]:\n",
    "    Dates.append(i.text)\n",
    "\n",
    "Times_tag=driver.find_elements_by_xpath(\"//div[@class ='match-card-left match-schedule']\")\n",
    "for i in Times_tag[0:10]:\n",
    "    Times.append(i.text)\n",
    "    \n",
    "match_data=pd.DataFrame({})\n",
    "match_data['Match_title']= Match_titles\n",
    "match_data['Series']= Seriess\n",
    "match_data['Place']= Places\n",
    "match_data['Date']= Dates\n",
    "match_data['Time']= Times\n",
    "\n",
    "\n",
    "convert = pd.DataFrame(match_data)\n",
    "convert.to_csv(\"match_data.csv\")\n",
    "\n",
    "a = pd.read_csv(\"match_data.csv\")\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b6ac573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    Repository_title  \\\n",
      "0           0   input-output-hk /   \n",
      "1           1          faker-js /   \n",
      "2           2             Marak /   \n",
      "3           3        cybersecsi /   \n",
      "4           4           QSCTech /   \n",
      "5           5  BoltzmannEntropy /   \n",
      "6           6        bevyengine /   \n",
      "7           7   input-output-hk /   \n",
      "8           8      521xueweihan /   \n",
      "9           9          Asabeneh /   \n",
      "\n",
      "                              Repository_description Contributors_count  \\\n",
      "0  Generate massive amounts of fake data in the b...              1,115   \n",
      "1                 get colors in your node.js console                466   \n",
      "2  A repo to automatically generate and keep upda...                970   \n",
      "3                                       浙江大学课程攻略共享计划                115   \n",
      "4  It is my belief that you the postgraduate stud...              4,644   \n",
      "5  A refreshingly simple data-driven game engine ...                414   \n",
      "6                    The Plutus application platform                251   \n",
      "7  分享 GitHub 上有趣、入门级的开源项目。Share interesting, entr...                 14   \n",
      "8  30 days of JavaScript programming challenge is...             24,623   \n",
      "9  WordPress Develop, Git-ified. Synced from git:...              7,266   \n",
      "\n",
      "  Language_used  \n",
      "0    JavaScript  \n",
      "1    JavaScript  \n",
      "2        Python  \n",
      "3          HTML  \n",
      "4          Rust  \n",
      "5       Haskell  \n",
      "6        Python  \n",
      "7    JavaScript  \n",
      "8           PHP  \n",
      "9            Go  \n"
     ]
    }
   ],
   "source": [
    "#Q5\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://github.com/')\n",
    "\n",
    "url = \"https://github.com/trending\"\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "Repository_titles = []\n",
    "Repository_descriptions = []\n",
    "Contributors_counts = []\n",
    "Language_useds = []\n",
    "\n",
    "titles=driver.find_elements_by_xpath(\"//span[@class ='text-normal']\")\n",
    "for i in titles[0:10]:\n",
    "    Repository_titles.append(i.text)\n",
    "    \n",
    "\n",
    "Repository_descriptions_tag=driver.find_elements_by_xpath(\"//p[@class ='col-9 color-fg-muted my-1 pr-4']\")\n",
    "for i in Repository_descriptions_tag[0:10]:\n",
    "    Repository_descriptions.append(i.text)\n",
    "    \n",
    "Contributors_counts_tag=driver.find_elements_by_xpath(\"//a[@class ='Link--muted d-inline-block mr-3']\")\n",
    "for i in Contributors_counts_tag[0:10]:\n",
    "    Contributors_counts.append(i.text)\n",
    "    \n",
    "Language_useds_tag=driver.find_elements_by_xpath(\"//span[@class ='d-inline-block ml-0 mr-3']\")\n",
    "for i in Language_useds_tag[0:10]:\n",
    "    Language_useds.append(i.text)\n",
    "    \n",
    "    \n",
    "github_data=pd.DataFrame({})\n",
    "github_data['Repository_title']= Repository_titles\n",
    "github_data['Repository_description']= Repository_descriptions\n",
    "github_data['Contributors_count']= Contributors_counts\n",
    "github_data['Language_used']= Language_useds\n",
    "\n",
    "\n",
    "convert = pd.DataFrame(github_data)\n",
    "convert.to_csv(\"github_data.csv\")\n",
    "\n",
    "a = pd.read_csv(\"github_data.csv\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a603ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://www.guru99.com/')\n",
    "\n",
    "\n",
    "url = 'https://www.guru99.com/exception-handling-selenium.html'\n",
    "driver.get(url)\n",
    "url = 'https://www.guru99.com/exception-handling-selenium.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "selenium_table= soup.find('table',class_ = 'table table-striped')\n",
    "for i in soup.find_all('tbody'):\n",
    "    rows = i.find_all('tr')\n",
    "    for row in rows:\n",
    "        excp_name=row.find_all('td')[2].text\n",
    "        print(excp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee8e3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Rank           State       GSDP      Share        GDP\n",
      "0           0     1     Maharashtra  2,632,792          -          -\n",
      "1           1     2      Tamil Nadu  1,630,208     13.94%     13.94%\n",
      "2           2     3   Uttar Pradesh  1,584,764    399.921    399.921\n",
      "3           3     4         Gujarat  1,502,899          -          -\n",
      "4           4     5       Karnataka  1,493,127  2,039,074  2,039,074\n",
      "5           5     6     West Bengal  1,089,898  1,845,853  1,845,853\n",
      "6           6     7       Rajasthan    942,586      8.63%      8.63%\n",
      "7           7     8  Andhra Pradesh    862,957    247.629    247.629\n",
      "8           8     9       Telangana    861,031  1,312,929  1,312,929\n",
      "9           9    10  Madhya Pradesh    809,592  1,215,307  1,215,307\n"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('http://statisticstimes.com/')\n",
    "\n",
    "eco_btn = driver.find_element_by_xpath(\"//i[@class='fa fa-caret-down']\")\n",
    "eco_btn.click()\n",
    "\n",
    "#driver.get('https://www.statisticstimes.com/economy/india-statistics.php')\n",
    "\n",
    "#state_btn = driver.find_element_by_xpath(\"//a[@class='ec']\")\n",
    "#state_btn.click()\n",
    "\n",
    "driver.get('https://www.statisticstimes.com/economy/india/indian-states-gdp.php')\n",
    "\n",
    "#url = 'https://www.statisticstimes.com/economy/india/indian-states-gdp.php'\n",
    "#response = requests.get(url)\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "#book_table= soup.find('table',class_ = 'display dataTable')\n",
    "\n",
    "#for team in book_table.find_all('tbody'):\n",
    " #   rows = team.find_all('tr')\n",
    "  #  for row in rows:\n",
    "   #     state_rank = row.find('td', class_ = 'data1').text.strip()\n",
    "        #book_name=row.find_all('td',class_ = 'left')[2].text\n",
    "    #    print(state_rank)\n",
    "                        \n",
    "Ranks=[]\n",
    "States=[]\n",
    "GSDPs=[]\n",
    "Shares=[]\n",
    "GDPs=[]\n",
    "\n",
    "Ranks_tag=driver.find_elements_by_xpath(\"//td[@class='data1']\")\n",
    "for i in Ranks_tag[0:10]:\n",
    "    Ranks.append(i.text)\n",
    "    \n",
    "States_tag=driver.find_elements_by_xpath(\"//td[@class='name']\")\n",
    "for i in States_tag[0:10]:\n",
    "    States.append(i.text)\n",
    "    \n",
    "GSDPs_tag=driver.find_elements_by_xpath(\"//td[@class='data sorting_1']\")\n",
    "for i in GSDPs_tag[0:10]:\n",
    "    GSDPs.append(i.text)\n",
    "    \n",
    "Shares_tag=driver.find_elements_by_xpath(\"//td[@class='data']\")\n",
    "for i in Shares_tag[0:10]:\n",
    "    Shares.append(i.text)\n",
    "    \n",
    "GDPs_tag=driver.find_elements_by_xpath(\"//td[@class='data']\")\n",
    "for i in GDPs_tag[0:10]:\n",
    "    GDPs.append(i.text)\n",
    "    \n",
    "gdp_data=pd.DataFrame({})\n",
    "gdp_data['Rank']= Ranks\n",
    "gdp_data['State']= States\n",
    "gdp_data['GSDP']= GSDPs\n",
    "gdp_data['Share']= Shares\n",
    "gdp_data['GDP']= GDPs \n",
    "\n",
    "convert = pd.DataFrame(gdp_data)\n",
    "convert.to_csv(\"gdp_data.csv\")\n",
    "\n",
    "a = pd.read_csv(\"gdp_data.csv\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "926d215e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   Song_name                                        Artist_name  \\\n",
      "0           0         NaN                      The Kid LAROI & Justin Bieber   \n",
      "1           1         NaN                                      Glass Animals   \n",
      "2           2         NaN                                         Ed Sheeran   \n",
      "3           3         NaN  Carolina Gaitan, Mauro Castillo, Adassa, Rhenz...   \n",
      "4           4         NaN                            Lil Nas X & Jack Harlow   \n",
      "5           5         NaN                              Elton John & Dua Lipa   \n",
      "6           6  Easy On Me                                         Ed Sheeran   \n",
      "7           7         NaN                                           Doja Cat   \n",
      "8           8         NaN                                        Kodak Black   \n",
      "9           9         NaN                                              GAYLE   \n",
      "\n",
      "   Last_week_rank  Peak_rank  Weeks_on_board  \n",
      "0           False      False           False  \n",
      "1           False      False           False  \n",
      "2           False      False           False  \n",
      "3           False      False           False  \n",
      "4           False      False           False  \n",
      "5           False      False           False  \n",
      "6           False      False           False  \n",
      "7           False      False           False  \n",
      "8           False      False           False  \n",
      "9           False      False           False  \n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Rutuja\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://www.billboard.com/charts/hot-100/')\n",
    "\n",
    "Song_names=[]\n",
    "Artist_names=[]\n",
    "Last_week_ranks=[]\n",
    "Peak_ranks=[]\n",
    "Weeks_on_boards=[]\n",
    "\n",
    "\n",
    "titles=driver.find_elements_by_id('title-of-a-story')\n",
    "for i in titles[0:10]:\n",
    "    Song_names.append(i.text)\n",
    "    \n",
    "Artist_names_tag=driver.find_elements_by_xpath(\"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only']\")\n",
    "for i in Artist_names_tag[0:10]:\n",
    "    Artist_names.append(i.text)\n",
    "    \n",
    "Last_week_ranks_tag=driver.find_elements_by_xpath(\"//span[@class='c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet']\")\n",
    "for i in Last_week_ranks_tag[0:10]:\n",
    "    Last_week_ranks.append(i.text)\n",
    "    \n",
    "Peak_ranks_tag=driver.find_elements_by_xpath(\"//span[@class='c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet']\")\n",
    "for i in Peak_ranks_tag[0:10]:\n",
    "    Peak_ranks.append(i.text)\n",
    "    \n",
    "Weeks_on_boards_tag=driver.find_elements_by_xpath(\"//span[@class='c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet']\")\n",
    "for i in Weeks_on_boards_tag[0:10]:\n",
    "    Weeks_on_boards.append(i.text)\n",
    "    \n",
    "song_data=pd.DataFrame({})\n",
    "song_data['Song_name']= Song_names\n",
    "song_data['Artist_name']= Artist_names\n",
    "song_data['Last_week_rank']= Last_week_ranks == 10\n",
    "song_data['Peak_rank']= Peak_ranks ==10\n",
    "song_data['Weeks_on_board']= Weeks_on_boards ==10\n",
    "\n",
    "\n",
    "convert = pd.DataFrame(song_data)\n",
    "convert.to_csv(\"song_data.csv\")\n",
    "\n",
    "a = pd.read_csv(\"song_data.csv\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1389b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
